<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="硫酸钾">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="https://white-troy.github.io/2024/04/22/大模型笔记/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="整体概述受限于显卡等硬件设备的限制，现阶段个人进行大模型训练基本都是在别人预训练好的大模型权重基础上进行微调。大模型训练主要可以概括为三个阶段1.预训练阶段，该阶段模型学习来自海量的，无标注的文本数据集2.使用SFT的方式对模型进行微调，从而使模型更好地遵守特定指令3.使用对齐方法让模型响应prompt 预训练在预训练阶段，LLM会从海量的文本中学习单词与单词的关系，即从传入的文本中预测下一个文本">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM大模型训练&amp;微调">
<meta property="og:url" content="https://white-troy.github.io/2024/04/22/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="整体概述受限于显卡等硬件设备的限制，现阶段个人进行大模型训练基本都是在别人预训练好的大模型权重基础上进行微调。大模型训练主要可以概括为三个阶段1.预训练阶段，该阶段模型学习来自海量的，无标注的文本数据集2.使用SFT的方式对模型进行微调，从而使模型更好地遵守特定指令3.使用对齐方法让模型响应prompt 预训练在预训练阶段，LLM会从海量的文本中学习单词与单词的关系，即从传入的文本中预测下一个文本">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://white-troy.github.io/2024/04/22/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/generate_mask.jpg">
<meta property="article:published_time" content="2024-04-22T13:54:58.832Z">
<meta property="article:modified_time" content="2024-04-30T06:51:47.897Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://white-troy.github.io/2024/04/22/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/generate_mask.jpg">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/redefine-favicon.svg" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/redefine-favicon.svg">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/redefine-favicon.svg">
    <!--- Page Info-->
    
    <title>
        
            LLM大模型训练&amp;微调 -
        
        硫酸钾的个人博客
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
<link rel="stylesheet" href="/fonts/fonts.css">

    
<link rel="stylesheet" href="/fonts/Satoshi/satoshi.css">

    
<link rel="stylesheet" href="/fonts/Chillax/chillax.css">

    <!--- Font Part-->
    
    
    
    

    <!--- Inject Part-->
    
    <script id="hexo-configurations">
    let Global = window.Global || {};
    Global.hexo_config = {"hostname":"white-troy.github.io","root":"/","language":"en"};
    Global.theme_config = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":true,"lazyload":false,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-valley.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"busuanzi_counter":{"enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"pjax":true,"open_graph":true,"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/wallhaven-valley.webp","dark":"/images/wallhaven-bocchi.webp"},"title":"welcome!","subtitle":{"text":[],"hitokoto":{"enable":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":false,"links":{"github":"https://github.com/white-troy","email":"zs1115996793@qq.com"}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null}]},"mermaid":{"enable":false,"version":"9.3.0"}},"version":"2.2.1","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"}},"search":{"enable":false,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"links":null},"article_date_format":"YYYY-MM-DD","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2022/8/17 11:45:14"};
    Global.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    Global.data_config = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
    <!--自定义看板娘-->
    <script src="https://cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script>
    <script src="/live2d-widget/autoload.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css"/>

<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="main-content-container">

        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                硫酸钾的个人博客
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        HOME
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer">
        <ul class="drawer-navbar-list">
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                HOME
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            
            
                <div class="article-title">
                    <h1 class="article-title-regular">LLM大模型训练&amp;微调</h1>
                </div>
            
                
            

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/redefine-avatar.svg">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">硫酸钾</span>
                            
                                <span class="author-label">Lv3</span>
                            
                        </div>
                        <div class="meta-info">
                            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2024-04-22 21:54:58</span>
        <span class="mobile">2024-04-22 21:54</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2024-04-30 14:51:47</span>
            <span class="mobile">2024-04-30 14:51</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content markdown-body">
                <h2 id="整体概述"><a href="#整体概述" class="headerlink" title="整体概述"></a>整体概述</h2><p>受限于显卡等硬件设备的限制，现阶段个人进行大模型训练基本都是在别人预训练好的大模型权重基础上进行微调。大模型训练主要可以概括为三个阶段<br>1.预训练阶段，该阶段模型学习来自海量的，无标注的文本数据集<br>2.使用SFT的方式对模型进行微调，从而使模型更好地遵守特定指令<br>3.使用对齐方法让模型响应prompt</p>
<h3 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h3><p>在预训练阶段，LLM会从海量的文本中学习单词与单词的关系，即从传入的文本中预测下一个文本（仅限于decoder-only，如果是bert则是类似于完形填空那种预测中间的单词），类似于续写的任务。该过程是一个无监督学习的过程，因为标签是文本的下一个词（从dataset代码中的[i]与[i+1]可以看出）。</p>
<h3 id="有监督微调"><a href="#有监督微调" class="headerlink" title="有监督微调"></a>有监督微调</h3><p>supervised finetune的数据集是基于人工标注的指令数据集，该阶段依然是一个预测下一个文本的训练过程，但输入是一个指令或者特殊的数据结构，如”<human>你好<endOfText>\n<bot>你好，我是一个大语言模型<endOfText>“，这种数据集，模型的输出为<bot>与<endOfText>之间的文本。在这个过程中，训练的目标是与预期得到的输出相同或接近。虽然预训练和SFT过程都是一个预测下一个文本的过程，但在SFT中，更多的是对LLM的输出进行一个规范，同时SFT的数据集是远小于预训练的数据集</endOfText></bot></endOfText></bot></endOfText></human></p>
<h3 id="Alignment对齐"><a href="#Alignment对齐" class="headerlink" title="Alignment对齐"></a>Alignment对齐</h3><p>这里的“对齐”的含义，其实是将模型微调到符合人类的三观或者偏好的过程，这个过程中会加入基于人类反馈的强化学习微调（RLHF），所以这个对齐过程也是一个微调的过程。</p>
<h4 id="预训练模型的有监督微调"><a href="#预训练模型的有监督微调" class="headerlink" title="预训练模型的有监督微调"></a>预训练模型的有监督微调</h4><p>在这个过程中，主要要收集许多prompt，然后由标注人员获取对应的结果，再以SFT的形式进行微调训练，从而让模型根据label学习到人类期望的回答偏好</p>
<h4 id="奖励模型"><a href="#奖励模型" class="headerlink" title="奖励模型"></a>奖励模型</h4><p>调整top-k/top-p/temperature等参数后，可以使模型获得许许多多不同的回答，根据这些回答，标注人员可以根据真实的偏好对回答进行打分（如：A回答&gt;B回答），接着根据这个排序训练奖励模型</p>
<h4 id="强化学习微调"><a href="#强化学习微调" class="headerlink" title="强化学习微调"></a>强化学习微调</h4><p>在此阶段，使用强化学习算法（如PPO）获得一个奖励函数来评估LLM生成的文本质量，通过损失函数来最大化每个不同的回答之间的差值，从而驱动LLM来生成更加符合人类喜好的回答</p>
<p>（refer:<a class="link" target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43243579/article/details/136205573">大模型训练流程（三）奖励模型-CSDN博客 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>）</p>
<h3 id="模型代码"><a href="#模型代码" class="headerlink" title="模型代码"></a>模型代码</h3><h4 id="LLM结构"><a href="#LLM结构" class="headerlink" title="LLM结构"></a>LLM结构</h4><h5 id="Transformer与encoder-decoder"><a href="#Transformer与encoder-decoder" class="headerlink" title="Transformer与encoder,decoder"></a>Transformer与encoder,decoder</h5><p>大多数LLM的结构都是基于transformer结构的。Transformer结构基本包括以下几个内容：</p>
<ol>
<li><strong>输入编码（Input Encoding）</strong>：<ul>
<li>将输入的文本序列通过词嵌入（Word Embedding）映射为词向量。</li>
</ul>
</li>
<li><strong>位置编码（Positional Encoding）</strong>：<ul>
<li>将位置信息加入到词向量中，以便模型能够理解词在序列中的顺序。</li>
</ul>
</li>
<li><strong>自注意力机制（Self-Attention Mechanism）</strong>：<ul>
<li>自注意力机制允许模型在处理每个词时都能考虑到输入序列中的所有其他词。通过计算注意力权重，模型可以决定在生成输出时应该关注哪些部分的输入。</li>
</ul>
</li>
<li><strong>多头注意力（Multi-Head Attention）</strong>：<ul>
<li>为了增强模型的表达能力，Transformer中通常使用多个自注意力头。每个注意力头都学习到一种不同的注意力表示，最后这些表示被拼接在一起并投影到输出空间。</li>
</ul>
</li>
<li><strong>残差连接（Residual Connection）和层归一化（Layer Normalization）</strong>：<ul>
<li>在每个子层（如自注意力层和前馈神经网络层）之后都会应用残差连接和层归一化，有助于减缓训练过程中的梯度消失问题，并提高模型的训练稳定性。</li>
</ul>
</li>
<li><strong>前馈神经网络（Feedforward Neural Network）</strong>：<ul>
<li>每个编码器层和解码器层都包含一个前馈神经网络。这个网络通常是一个两层的全连接神经网络，用于对注意力表示进行非线性变换。</li>
</ul>
</li>
</ol>
<p>在transformer模型中，encoder负责理解与提取输入的相关信息，并用自注意力机制来理解输入的内容中每个元素之间的关系，encoder的输出是高维向量（用高维度的tensor来表示输入的一些特征），这个向量输出给decoder，decoder再使用自注意力与编解码注意力机制对输入的高维向量进行处理。它的输出是每个元素的概率分布。如输入是文本的情况中，在给定上下文条件下，下一个词是词汇表中每个词的可能性。通常情况下，模型会选择具有最高概率的词作为生成的下一个词，然后将其作为输入传递给下一个时间步骤，直到生成特殊的终止标记或达到最大输出长度为止。这样就完成了生成序列的过程。</p>
<p>典型的transformer结构包括了encoder模块与decoder模块，而也有一些模型仅使用了其中的一部分，这就构成了三种不同的LLM框架：encoder-only,decoder-only,encoder-decoder (refer:<a class="link" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/642923989">LLM的3种架构：Encoder-only、Decoder-only、encode-decode - 知乎 (zhihu.com) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>)</p>
<h6 id="encoder-only"><a href="#encoder-only" class="headerlink" title="encoder-only"></a>encoder-only</h6><p>encoder-only的模型架构更擅长做分类任务，如对文本进行分析，情感分类等。如Bert，他的训练是基于next-sentence prediction task和mask language modeling。在训练的过程中会将句子进行掩码操作与打乱顺序，让bert根据上下文的内容来预测被掩码的词是什么，并且判断句子是否被打乱顺序。</p>
<h6 id="decoder-only"><a href="#decoder-only" class="headerlink" title="decoder-only"></a>decoder-only</h6><p>decoder-only的模型架构与encoder-only类似，但是在位置position上用到了mask。自注意力机制允许每个位置的输出都依赖于输入序列中所有位置的信息。然而，当生成输出序列（例如在文本翻译任务中生成目标语言的文本）时，我们希望位置 <em>i</em> 的输出只依赖于位置 <em>i</em> 之前的已知输出，而不依赖于位置 <em>i</em> 之后的输出。所以将后面的内容进行了掩码操作，从而让模型更多的关注上文的信息。如LLaMa,GPT</p>
<h6 id="encoder-decoder"><a href="#encoder-decoder" class="headerlink" title="encoder-decoder"></a>encoder-decoder</h6><p>这种架构的LLM通常充分利用了上面2种类型的优势，采用新的技术和架构调整来优化表现。这种主要用于NLP，即理解输入的内容NLU，又能处理并生成内容NLG，尤其擅长处理输入和输出序列之间<strong>存在复杂映射关系</strong>的任务，以及捕捉两个序列中元素之间关系至关重要的任务。如BART与T5</p>
<h5 id="附加：为什么现在的LLM都是decoder-only的架构？"><a href="#附加：为什么现在的LLM都是decoder-only的架构？" class="headerlink" title="附加：为什么现在的LLM都是decoder only的架构？"></a>附加：为什么现在的LLM都是decoder only的架构？</h5><p>refer:<a class="link" target="_blank" rel="noopener" href="https://www.zhihu.com/question/588325646/answer/3383505083">为什么现在的LLM都是Decoder only的架构？ - 知乎 (zhihu.com) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p><img src="/2024/04/22/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/generate_mask.jpg" alt="模型的生成方向"></p>
<p>以BERT为代表的encoder-only、以T5和BART为代表的encoder-decoder、以GPT为代表的decoder-only，还有以UNILM为代表的PrefixLM（相比于GPT只改了attention mask，前缀部分是双向，后面要生成的部分是单向的causal mask)</p>
<p>对于bert的结构，这是用masked language modeling进行训练，类似于填写完形填空，所以在生成任务上会略逊色于decoder-only，同时对于微调也需要一定的有监督数据；而对于GPT这种decoder-only的结构，是使用next token prediction的训练方式，输入文本的下一个词就类似于标签。同时，在兼顾上下文时，需要同时计算两个注意力矩阵（计算当前时刻往前的为上三角矩阵，计算当前时刻往后的为下三角矩阵，同时计算前后则是一个满的矩阵），为了减小复杂度可能会进行低秩处理，而低秩则会导致当前区间小于输入序列的长度，仅关注到一个相对比较小的区间，从而无法像满秩矩阵（秩与输入序列长度一样）一样为每个位置分配一个注意力权重。</p>
<p>此外，在效率问题上，decoder-only支持复用kv-cache，对于多轮对话更友好，因为每个token的表示只和它之前的输入有关。</p>
<h4 id="LLM训练"><a href="#LLM训练" class="headerlink" title="LLM训练"></a>LLM训练</h4><p>LLM的训练过程大致上与其他模型类似，都是经过数据集准备，模型搭建，参数设置，训练调整。下面以<a class="link" target="_blank" rel="noopener" href="https://github.com/karpathy/nanoGPT">nanoGPT <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>的训练为例</p>
<h5 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h5><p>数据的准备一般需要经过清洗，token化，添加特殊标记，创建训练样本，序列化等步骤。在清洗中，一般是清洗掉多余的元素（如emoji表情，空格等非文本元素）并进行规范化。然后使用模型的tokenizer对文本进行token化（一般使用BPE或SentencePiece，这有助于处理未知词汇和减少词汇表大小），然后在文段末尾添加停止符（基于nanoGPT，添加<endOfText>作为文段停止标志）。</endOfText></p>
<h5 id="模型搭建"><a href="#模型搭建" class="headerlink" title="模型搭建"></a>模型搭建</h5><p>一个LLM模型主要由LayerNorm层，注意力层，MLP层，embedding层组成，这其中最重要的就是注意力层，不同结构的注意力操作对模型性能会带来不同的结果。在写这篇笔记的同时我也基于nanoGPT训练一个小型的LLM，后续的计划是更换不同的注意力机制，对比看是否会有不同的效果。模型的结构可以参考<a class="link" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/338817680">Transformer模型详解（图解最完整版） - 知乎 (zhihu.com) <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h5 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h5><p>在nanoGPT中，主要可以设置的参数是“模型结构参数”与“训练参数”，其中模型结构参数可以设置：block_size(上下文长度),n_layer(模型堆叠层数),n_head(模型注意力头的个数),n_embed(嵌入向量的维度),bias(是否使用偏置项),dropout(dropout的比率)。</p>
<p><code>block_size</code>（也称为上下文长度或窗口大小）在处理长文本和理解语言上下文的连贯性方面对模型性能有着显著的影响，。<code>block_size</code>决定了模型在一次前向传播中可以考虑的最大文本长度。较大的<code>block_size</code>使模型能够处理更长的依赖关系，理解更复杂的语言结构和上下文。但是这也会直接影响到模型训练时的内存消耗与计算负载。</p>
<p><code>n_embed</code>，即嵌入层的维度或模型的隐藏维数，是定义大型语言模型（如GPT、BERT等）中每个单词向量的维度大小，这个大小也是反映了输入词的特征维数，更大的维数代表了单词会被投影到更高维的空间，从而能带来更多的语义特征。它对模型的表达能力，过拟合与泛化能力都有不同程度的影响。</p>
<h4 id="LoRA微调"><a href="#LoRA微调" class="headerlink" title="LoRA微调"></a>LoRA微调</h4><p>LoRA微调是将大模型的其他参数固定，引入额外的可学习的低秩矩阵来调整模型中的关键参数，进而达到微调的效果。主要是对模型中的Linear层进行修改，特别是注意力层中的nn.Linear层。</p>
<p>针对注意力层调整的原因主要是：在注意力机制中，qkv通过线性层进行投影，而Q层和V层会直接影响到注意力权重的计算以及最终的输出表示。通过Q的调整，可以影响模型如何选择信息（即“注意”哪些信息并选择），通过V的调整，则会影响一旦选择了某些信息，模型会如何利用这些信息，通过K的调整，会影响模型如何进行信息的匹配。</p>
<p>在写这篇笔记的时候，我暂时只进行了模型的预训练与lora微调，后续还会对LLM模型的其他技术进行学习（如RAG,Agent等），待学习完毕会更新另外对应的笔记</p>

            </div>

            
                <div class="post-copyright-info">
                    <div class="article-copyright-info-container">
    <ul>
        <li><strong>Title:</strong> LLM大模型训练&amp;微调</li>
        <li><strong>Author:</strong> 硫酸钾</li>
        <li><strong>Created at:</strong> 2024-04-22 21:54:58</li>
        
            <li>
                <strong>Updated at:</strong> 2024-04-30 14:51:47
            </li>
        
        <li>
            <strong>Link:</strong> https://white-troy.github.io/2024/04/22/大模型笔记/
        </li>
        <li>
            <strong>License:</strong> This work is licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0</a>.
        </li>
    </ul>
</div>

                </div>
            

            

            

            
                <div class="article-nav">
                    
                        <div class="article-prev">
                            <a class="prev"
                            rel="prev"
                            href="/2024/05/28/tokenizer/"
                            >
                                <span class="left arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-left"></i>
                                </span>
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">LLM的tokenizer</span>
                                    <span class="post-nav-item">Prev posts</span>
                                </span>
                            </a>
                        </div>
                    
                    
                        <div class="article-next">
                            <a class="next"
                            rel="next"
                            href="/2024/03/18/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95/"
                            >
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">大模型数据库相关算法</span>
                                    <span class="post-nav-item">Next posts</span>
                                </span>
                                <span class="right arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-right"></i>
                                </span>
                            </a>
                        </div>
                    
                </div>
            


            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">On this page</div>
        <div class="page-title">LLM大模型训练&amp;微调</div>
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B4%E4%BD%93%E6%A6%82%E8%BF%B0"><span class="nav-text">整体概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="nav-text">预训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%89%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83"><span class="nav-text">有监督微调</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Alignment%E5%AF%B9%E9%BD%90"><span class="nav-text">Alignment对齐</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81"><span class="nav-text">模型代码</span></a></li></ol></li></ol>

    </div>
</div>
            </div>
        
    </div>
</div>


                

            </div>
            
            

        </div>

        <div class="main-content-footer">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info">
            &copy;
            
              <span>2022</span>
              -
            
            2024&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">硫酸钾</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv" class="busuanzi_container_site_uv">
                        VISITOR COUNT&nbsp;<span id="busuanzi_value_site_uv" class="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="busuanzi_container_site_pv">
                        TOTAL PAGE VIEWS&nbsp;<span id="busuanzi_value_site_pv" class="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            <span class="powered-by-container">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" href="https://hexo.io">Hexo</a></span>
                <br>
            <span class="theme-version-container">THEME&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.2.1</a>
        </div>
        
        
        
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
        
            <script async data-pjax>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    


</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/layouts/navbarShrink.js"></script>

<script src="/js/tools/scrollTopBottom.js"></script>

<script src="/js/tools/lightDarkSwitch.js"></script>





    
<script src="/js/tools/codeBlock.js"></script>






    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js"></script>







<div class="post-scripts pjax">
    
        
<script src="/js/tools/tocToggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/layouts/toc.js"></script>

<script src="/js/plugins/tabs.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax',
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            Global.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            Global.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            Global.refresh();
        });
    });
</script>




</body>
</html>
